{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/data2/xianglin/cache/huggingface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'sharegpt',\n",
       " 'instruction': 'how to say someone is better without saying smoeone is better',\n",
       " 'output': 'The best way to imply someone is better without directly saying it depends heavily on the context. Here are some options, categorized by what you want to emphasize:\\n\\n**Focusing on their skills or achievements:**\\n\\n* \"They exceeded expectations.\"\\n* \"Their work is truly exceptional.\"\\n* \"They\\'re a natural at this.\"\\n* \"They have a remarkable talent for...\"\\n* \"Their contribution was invaluable.\"\\n* \"They consistently deliver high-quality results.\"\\n* \"They\\'re a real asset to the team.\"\\n* \"Their performance was outstanding.\"\\n* \"They\\'ve really mastered this skill.\"\\n* \"They\\'re ahead of the curve.\"\\n\\n\\n**Focusing on their character or qualities:**\\n\\n* \"They have a great work ethic.\"\\n* \"They\\'re incredibly dedicated.\"\\n* \"They\\'re a real problem-solver.\"\\n* \"They\\'re very insightful.\"\\n* \"They\\'re incredibly resourceful.\"\\n* \"They have a unique perspective.\"\\n* \"They\\'re a joy to work with.\"\\n* \"They\\'re exceptionally reliable.\"\\n* \"They\\'re a natural leader.\"\\n\\n\\n**Focusing on the comparison (more subtly):**\\n\\n* \"Their approach is more effective.\" (compared to someone else\\'s)\\n* \"They produced a superior result.\"\\n* \"Their method is more efficient.\"\\n* \"Their solution was more innovative.\"\\n\\n\\n**Example:** Instead of saying \"John is a better programmer than Mary,\" you could say, \"John\\'s code is remarkably efficient and clean, consistently exceeding project deadlines.\"\\n\\n\\nRemember to choose the phrasing that best fits the situation and your relationship with the people involved.  Direct comparisons can often be hurtful or create unnecessary competition.  Subtlety is often more effective.'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_gemini_LlamaFactory.json\"\n",
    "data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_gpt4_LlamaFactory.json\"\n",
    "data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_llama_LlamaFactory.json\"\n",
    "\n",
    "import json\n",
    "with open(data_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_question = data[0]['instruction']\n",
    "example_text = data[0]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_features import extract_surface_formatting_features_vector\n",
    "feature_vec = extract_surface_formatting_features_vector(example_text)\n",
    "\n",
    "print(\"Feature Vector:\", feature_vec)\n",
    "\n",
    "empty_vec = extract_surface_formatting_features_vector(\"\")\n",
    "print(\"\\nFeatures for empty text:\", empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_features import extract_lexical_features_vector\n",
    "\n",
    "lexical_vec = extract_lexical_features_vector(example_text)\n",
    "print(\"Lexical Feature Vector:\", lexical_vec)\n",
    "\n",
    "empty_vec = extract_lexical_features_vector(\"\")\n",
    "print(\"\\nFeatures for empty text:\", empty_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_features import extract_pos_features_vector\n",
    "pos_names, pos_vec = extract_pos_features_vector(example_text)\n",
    "print(\"\\nPOS Feature Names:\", pos_names)\n",
    "print(\"POS Feature Vector:\", pos_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_features import extract_syntactic_features_vector\n",
    "core_vec, dep_vec, dep_names = extract_syntactic_features_vector(example_text, include_dep_ratios=True)\n",
    "\n",
    "print(\"Core Syntactic Feature Vector:\", [f\"{x:.3f}\" for x in core_vec]) # Format for readability\n",
    "\n",
    "print(\"\\nDependency Ratio Names:\", dep_names)\n",
    "print(\"Dependency Ratio Vector:\", [f\"{x:.3f}\" for x in dep_vec]) # Format for readability\n",
    "\n",
    "# Example without dependency ratios\n",
    "core_vec_only, _, _ = extract_syntactic_features_vector(example_text, include_dep_ratios=False)\n",
    "print(\"\\nCore Syntactic Feature Vector (Only):\", [f\"{x:.3f}\" for x in core_vec_only])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "from src.extract_features import extract_discourse_structural_features_vector\n",
    "discourse_vec = extract_discourse_structural_features_vector(example_text)\n",
    "\n",
    "print(\"Discourse Feature Vector:\", [f\"{x:.3f}\" for x in discourse_vec])\n",
    "\n",
    "# Example with empty text:\n",
    "empty_vec = extract_discourse_structural_features_vector(\"\")\n",
    "print(\"\\nFeatures for empty text:\", empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6515e6ea9bf48b38181699abc41dbeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'meta-llama/Llama-3.1-8B-Instruct' loaded on cuda:0.\n",
      "\n",
      "Features for Answer GIVEN Question:\n",
      "Feature Vector: ['3.65625', '1.296875', '-1.296875', '0.7421875', '15.0625', '[0.0002994537353515625, 0.0002994537353515625, 0.0002956390380859375, 0.000324249267578125, 1.1484375, 1.7265625, 0.90234375, 0.0024261474609375, 1.96875, 0.65625, 0.279296875, 0.59765625, 0.0810546875, 0.50390625, 0.27734375, 1.234375, 1.1328125, 0.000537872314453125, 0.00750732421875, 0.40625, 0.244140625, 0.61328125, 0.58984375, 0.00159454345703125, 0.62890625, 1.78125, 0.80859375, 3.890625, 1.265625, 2.671875, 0.74609375, 1.046875, 0.0018310546875, 0.875, 0.82421875, 0.92578125, 3.109375, 0.796875, 0.000518798828125, 2.5, 2.125, 0.71484375, 1.21875, 0.34765625, 0.12255859375, 0.1806640625, 0.515625, 1.109375, 1.3671875, 0.60546875, 1.1875, 0.00038909912109375, 0.01263427734375, 0.75390625, 1.484375, 3.578125, 2.890625, 0.9375, 0.0732421875, 0.000308990478515625, 0.004730224609375, 1.0078125, 2.1875, 3.0, 1.6796875, 0.89453125, 0.6875, 0.0908203125, 0.00034332275390625, 0.0101318359375, 0.5703125, 1.984375, 0.205078125, 2.234375, 0.48828125, 0.83203125, 1.5390625, 0.0004558563232421875, 0.0120849609375, 0.408203125, 1.7109375, 1.1796875, 0.7734375, 0.82421875, 0.001129150390625, 0.00762939453125, 0.765625, 3.078125, 0.62890625, 0.47265625, 0.030517578125, 0.09912109375, 0.6328125, 0.000762939453125, 0.00848388671875, 0.71484375, 1.703125, 1.828125, 2.140625, 1.6640625, 0.267578125, 0.10888671875, 0.012939453125, 0.490234375, 0.000720977783203125, 0.00677490234375, 0.41796875, 1.6015625, 1.6640625, 0.578125, 0.361328125, 0.001190185546875, 0.006500244140625, 0.396484375, 2.75, 2.171875, 2.546875, 0.734375, 0.8984375, 0.48828125, 0.00160980224609375, 0.006561279296875, 0.65625, 2.109375, 2.03125, 0.0177001953125, 0.25, 0.310546875, 0.75, 0.056884765625, 1.453125, 0.005615234375, 0.0006561279296875, 0.96875, 3.5625, 0.392578125, 2.109375, 0.003570556640625, 0.0179443359375, 0.00096893310546875, 0.019775390625, 0.06591796875, 1.21875, 0.2734375, 2.125, 0.75390625, 0.003631591796875, 0.05322265625, 0.0003509521484375, 0.0019683837890625, 0.2412109375, 0.083984375, 2.078125, 0.84375, 0.6328125, 0.00032806396484375, 0.0004482269287109375, 0.32421875, 0.96875, 1.578125, 1.6328125, 1.3046875, 0.3984375, 0.00335693359375, 0.017333984375, 0.00034332275390625, 0.00131988525390625, 0.1240234375, 0.8515625, 1.9921875, 3.234375, 0.19140625, 0.00037384033203125, 0.00189971923828125, 0.05810546875, 0.81640625, 1.6640625, 3.09375, 0.0003261566162109375, 0.011474609375, 0.0004215240478515625, 0.0019378662109375, 0.04736328125, 0.75, 0.4375, 2.375, 0.12890625, 0.1943359375, 0.000705718994140625, 0.002227783203125, 0.0252685546875, 0.01397705078125, 1.5625, 1.90625, 0.0002994537353515625, 0.076171875, 0.000652313232421875, 0.076171875, 0.00107574462890625, 0.00238037109375, 0.042724609375, 0.384765625, 1.40625, 1.96875, 0.26953125, 0.00250244140625, 0.0031890869140625, 0.028076171875, 0.76171875, 1.4921875, 1.6875, 0.0634765625, 0.8359375, 0.02734375, 0.58984375, 0.00201416015625, 0.0005645751953125, 1.703125, 1.953125, 1.65625, 1.2734375, 2.671875, 0.32421875, 0.004608154296875, 0.003082275390625, 0.018798828125, 2.125, 1.65625, 1.5234375, 2.578125, 0.88671875, 0.578125, 0.458984375, 3.265625, 0.453125, 0.00494384765625, 1.4609375, 0.2021484375, 0.84375, 1.1640625, 0.00046539306640625, 0.005523681640625, 1.2578125, 1.7734375, 1.578125, 1.28125, 0.62890625, 0.71875, 0.00830078125, 0.0086669921875, 0.3984375, 2.109375, 1.65625, 0.84375, 0.287109375, 0.55078125, 0.000957489013671875, 0.0062255859375, 0.82421875, 2.890625, 0.75390625, 0.427734375, 1.3203125, 0.259765625, 0.859375, 2.953125, 0.76171875, 2.703125, 0.000873565673828125, 0.197265625, 0.75, 1.53125, 0.061279296875, 0.65234375, 0.08740234375, 2.625, 0.3671875, 2.53125, 0.376953125, 0.349609375, 0.314453125, 0.0284423828125, 1.203125, 0.060791015625, 0.0283203125, 0.54296875, 1.2421875, 0.96875, 0.94921875, 0.74609375, 1.5078125, 2.984375, 1.53125, 1.9765625, 2.40625, 1.59375, 1.5078125, 1.3984375, 2.78125, 1.1015625, 2.15625, 1.6328125, 2.375, 0.0029144287109375, 0.20703125, 0.8984375, 0.7578125, 0.58203125, 1.0234375, 0.46484375, 2.0, 1.4453125, 0.00213623046875, 0.03662109375, 0.45703125, 0.0986328125, 0.15234375, 1.828125, 4.03125, 2.015625, 0.92578125, 1.109375, 1.2890625, 2.234375, 0.00299072265625, 0.71484375, 3.046875, 2.0, 1.1015625, 0.95703125, 1.890625, 3.828125, 0.6953125, 0.84765625, 0.021484375, 1.4765625, 1.4375, 1.546875, 1.0390625, 1.6171875, 0.05908203125]', '[1.0, 1.0, 1.0, 1.0, 3.15625, 5.625, 2.46875, 1.0, 7.15625, 1.9296875, 1.3203125, 1.8203125, 1.0859375, 1.65625, 1.3203125, 3.4375, 3.109375, 1.0, 1.0078125, 1.5, 1.2734375, 1.84375, 1.8046875, 1.0, 1.875, 5.9375, 2.25, 49.0, 3.546875, 14.4375, 2.109375, 2.84375, 1.0, 2.40625, 2.28125, 2.53125, 22.375, 2.21875, 1.0, 12.1875, 8.375, 2.046875, 3.390625, 1.4140625, 1.1328125, 1.1953125, 1.671875, 3.03125, 3.921875, 1.8359375, 3.28125, 1.0, 1.015625, 2.125, 4.40625, 35.75, 18.0, 2.546875, 1.078125, 1.0, 1.0078125, 2.734375, 8.9375, 20.125, 5.375, 2.453125, 1.9921875, 1.09375, 1.0, 1.0078125, 1.765625, 7.28125, 1.2265625, 9.3125, 1.6328125, 2.296875, 4.65625, 1.0, 1.015625, 1.5078125, 5.53125, 3.25, 2.171875, 2.28125, 1.0, 1.0078125, 2.15625, 21.75, 1.875, 1.6015625, 1.03125, 1.1015625, 1.8828125, 1.0, 1.0078125, 2.046875, 5.5, 6.21875, 8.5, 5.28125, 1.3046875, 1.1171875, 1.015625, 1.6328125, 1.0, 1.0078125, 1.515625, 4.96875, 5.28125, 1.78125, 1.4375, 1.0, 1.0078125, 1.484375, 15.625, 8.75, 12.75, 2.078125, 2.453125, 1.6328125, 1.0, 1.0078125, 1.9296875, 8.25, 7.625, 1.015625, 1.28125, 1.3671875, 2.109375, 1.0546875, 4.28125, 1.0078125, 1.0, 2.640625, 35.25, 1.484375, 8.25, 1.0, 1.015625, 1.0, 1.0234375, 1.0703125, 3.390625, 1.3125, 8.375, 2.125, 1.0, 1.0546875, 1.0, 1.0, 1.2734375, 1.0859375, 8.0, 2.328125, 1.8828125, 1.0, 1.0, 1.3828125, 2.640625, 4.84375, 5.125, 3.6875, 1.4921875, 1.0, 1.015625, 1.0, 1.0, 1.1328125, 2.34375, 7.34375, 25.375, 1.2109375, 1.0, 1.0, 1.0625, 2.265625, 5.28125, 22.0, 1.0, 1.0078125, 1.0, 1.0, 1.046875, 2.109375, 1.546875, 10.75, 1.140625, 1.2109375, 1.0, 1.0, 1.0234375, 1.015625, 4.78125, 6.71875, 1.0, 1.078125, 1.0, 1.078125, 1.0, 1.0, 1.046875, 1.46875, 4.09375, 7.15625, 1.3125, 1.0, 1.0, 1.03125, 2.140625, 4.4375, 5.40625, 1.0625, 2.3125, 1.03125, 1.8046875, 1.0, 1.0, 5.5, 7.0625, 5.25, 3.578125, 14.4375, 1.3828125, 1.0078125, 1.0, 1.015625, 8.375, 5.25, 4.59375, 13.1875, 2.421875, 1.78125, 1.5859375, 26.25, 1.5703125, 1.0078125, 4.3125, 1.2265625, 2.328125, 3.203125, 1.0, 1.0078125, 3.515625, 5.90625, 4.84375, 3.59375, 1.875, 2.046875, 1.0078125, 1.0078125, 1.4921875, 8.25, 5.25, 2.328125, 1.3359375, 1.734375, 1.0, 1.0078125, 2.28125, 18.0, 2.125, 1.53125, 3.75, 1.296875, 2.359375, 19.125, 2.140625, 14.9375, 1.0, 1.21875, 2.109375, 4.625, 1.0625, 1.921875, 1.09375, 13.8125, 1.4453125, 12.5625, 1.4609375, 1.421875, 1.3671875, 1.03125, 3.328125, 1.0625, 1.03125, 1.71875, 3.46875, 2.640625, 2.578125, 2.109375, 4.53125, 19.75, 4.625, 7.21875, 11.0625, 4.9375, 4.53125, 4.0625, 16.125, 3.015625, 8.625, 5.125, 10.75, 1.0, 1.2265625, 2.453125, 2.140625, 1.7890625, 2.78125, 1.59375, 7.375, 4.25, 1.0, 1.0390625, 1.578125, 1.1015625, 1.1640625, 6.21875, 56.25, 7.5, 2.53125, 3.03125, 3.625, 9.3125, 1.0, 2.046875, 21.0, 7.375, 3.015625, 2.609375, 6.625, 46.0, 2.0, 2.328125, 1.0234375, 4.375, 4.21875, 4.6875, 2.828125, 5.03125, 1.0625]', '[0.0, 0.0, 0.0, 0.0, 0.5546875, 0.61328125, 0.3984375, 0.0, 0.75, 0.44140625, 0.11328125, 0.22265625, 0.0234375, 0.1953125, 0.1328125, 0.65234375, 0.56640625, 0.0, 0.0, 0.20703125, 0.109375, 0.3515625, 0.25, 0.0, 0.40234375, 0.64453125, 0.41015625, 0.9453125, 0.59375, 0.8671875, 0.2421875, 0.52734375, 0.0, 0.34375, 0.37890625, 0.55859375, 0.890625, 0.37109375, 0.0, 0.80078125, 0.7734375, 0.3359375, 0.55859375, 0.16796875, 0.0546875, 0.078125, 0.2265625, 0.4453125, 0.5703125, 0.28125, 0.609375, 0.0, 0.0, 0.4921875, 0.671875, 0.90625, 0.90234375, 0.4609375, 0.0234375, 0.0, 0.0, 0.55859375, 0.7890625, 0.8671875, 0.625, 0.3828125, 0.30078125, 0.0234375, 0.0, 0.0, 0.30078125, 0.7890625, 0.0625, 0.828125, 0.1796875, 0.51953125, 0.6015625, 0.0, 0.0, 0.19921875, 0.6875, 0.546875, 0.29296875, 0.484375, 0.0, 0.0, 0.4765625, 0.91796875, 0.22265625, 0.20703125, 0.0078125, 0.03125, 0.43359375, 0.0, 0.0, 0.48828125, 0.70703125, 0.640625, 0.7578125, 0.703125, 0.10546875, 0.03125, 0.0, 0.2109375, 0.0, 0.0, 0.234375, 0.6796875, 0.6953125, 0.28125, 0.18359375, 0.0, 0.0, 0.2109375, 0.87109375, 0.76953125, 0.87109375, 0.36328125, 0.35546875, 0.21484375, 0.0, 0.0, 0.42578125, 0.7265625, 0.71875, 0.0078125, 0.1171875, 0.1328125, 0.38671875, 0.0078125, 0.640625, 0.0, 0.0, 0.328125, 0.94921875, 0.18359375, 0.8203125, 0.0, 0.0078125, 0.0, 0.0078125, 0.0234375, 0.57421875, 0.09765625, 0.7734375, 0.390625, 0.0, 0.015625, 0.0, 0.0, 0.1171875, 0.0234375, 0.8203125, 0.296875, 0.328125, 0.0, 0.0, 0.1796875, 0.44140625, 0.65625, 0.6015625, 0.625, 0.2265625, 0.0, 0.0078125, 0.0, 0.0, 0.0546875, 0.44921875, 0.77734375, 0.91015625, 0.0859375, 0.0, 0.0, 0.0234375, 0.484375, 0.66015625, 0.91796875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.4375, 0.1875, 0.8515625, 0.0390625, 0.0859375, 0.0, 0.0, 0.0078125, 0.0, 0.671875, 0.76171875, 0.0, 0.0234375, 0.0, 0.0234375, 0.0, 0.0, 0.015625, 0.18359375, 0.5546875, 0.6640625, 0.10546875, 0.0, 0.0, 0.0078125, 0.4453125, 0.6484375, 0.671875, 0.015625, 0.4140625, 0.0078125, 0.234375, 0.0, 0.0, 0.7109375, 0.7265625, 0.64453125, 0.359375, 0.83984375, 0.10546875, 0.0, 0.0, 0.0, 0.8046875, 0.5390625, 0.54296875, 0.8515625, 0.53125, 0.26171875, 0.19140625, 0.921875, 0.2734375, 0.0, 0.6328125, 0.0625, 0.50390625, 0.578125, 0.0, 0.0, 0.6171875, 0.71875, 0.63671875, 0.5078125, 0.40234375, 0.46484375, 0.0, 0.0, 0.16015625, 0.7890625, 0.6484375, 0.24609375, 0.09765625, 0.33203125, 0.0, 0.0, 0.50390625, 0.875, 0.4921875, 0.14453125, 0.6484375, 0.109375, 0.26953125, 0.81640625, 0.265625, 0.87890625, 0.0, 0.078125, 0.4453125, 0.5625, 0.015625, 0.3359375, 0.0234375, 0.83984375, 0.1953125, 0.875, 0.1796875, 0.15234375, 0.140625, 0.0078125, 0.5703125, 0.015625, 0.0078125, 0.21484375, 0.5703125, 0.4921875, 0.32421875, 0.27734375, 0.73828125, 0.875, 0.71875, 0.76171875, 0.8125, 0.6953125, 0.73046875, 0.609375, 0.859375, 0.6484375, 0.8203125, 0.7265625, 0.8671875, 0.0, 0.078125, 0.4296875, 0.4453125, 0.390625, 0.5703125, 0.2578125, 0.76171875, 0.6171875, 0.0, 0.0078125, 0.18359375, 0.0390625, 0.0625, 0.58203125, 0.95703125, 0.765625, 0.34765625, 0.5390625, 0.640625, 0.7578125, 0.0, 0.36328125, 0.83984375, 0.76171875, 0.59375, 0.5234375, 0.58203125, 0.94140625, 0.4921875, 0.53125, 0.0078125, 0.703125, 0.671875, 0.6875, 0.34765625, 0.74609375, 0.015625]', '[11.75, 11.75, 11.75, 11.75, 10.5625, 10.0, 10.875, 11.75, 9.75, 11.125, 11.4375, 11.125, 11.6875, 11.25, 11.5, 10.5625, 10.625, 11.75, 11.75, 11.3125, 11.5, 11.1875, 11.1875, 11.75, 11.1875, 9.9375, 10.875, 7.875, 10.5, 9.0625, 11.0, 10.6875, 11.75, 10.9375, 10.875, 10.8125, 8.625, 10.9375, 11.75, 9.25, 9.625, 11.0, 10.5, 11.3125, 11.625, 11.5625, 11.25, 10.625, 10.375, 11.0625, 10.5625, 11.75, 11.75, 11.0, 10.3125, 8.1875, 8.875, 10.8125, 11.6875, 11.75, 11.75, 10.75, 9.5625, 8.75, 10.0625, 10.8125, 11.0625, 11.6875, 11.75, 11.75, 11.1875, 9.75, 11.5, 9.5, 11.25, 10.9375, 10.1875, 11.75, 11.75, 11.3125, 10.0, 10.5625, 11.0, 10.9375, 11.75, 11.75, 10.9375, 8.6875, 11.0625, 11.3125, 11.6875, 11.625, 11.125, 11.75, 11.75, 11.0625, 10.0625, 9.9375, 9.625, 10.0625, 11.5, 11.625, 11.75, 11.25, 11.75, 11.75, 11.375, 10.125, 10.0625, 11.1875, 11.3125, 11.75, 11.75, 11.375, 8.9375, 9.5625, 9.1875, 11.0, 10.875, 11.25, 11.75, 11.75, 11.0625, 9.6875, 9.6875, 11.6875, 11.5, 11.5, 11.0, 11.6875, 10.25, 11.75, 11.75, 10.8125, 8.1875, 11.3125, 9.625, 11.75, 11.6875, 11.75, 11.6875, 11.6875, 10.5625, 11.5, 9.625, 10.9375, 11.75, 11.75, 11.75, 11.75, 11.5, 11.6875, 9.6875, 10.875, 11.1875, 11.75, 11.75, 11.375, 10.8125, 10.125, 10.125, 10.4375, 11.375, 11.75, 11.6875, 11.75, 11.75, 11.625, 10.875, 9.75, 8.5, 11.5625, 11.75, 11.75, 11.6875, 10.9375, 10.125, 8.625, 11.75, 11.75, 11.75, 11.75, 11.75, 11.0, 11.3125, 9.375, 11.625, 11.5625, 11.75, 11.75, 11.6875, 11.75, 10.1875, 9.875, 11.75, 11.6875, 11.75, 11.6875, 11.75, 11.75, 11.75, 11.375, 10.3125, 9.75, 11.5, 11.75, 11.75, 11.6875, 10.9375, 10.1875, 10.0625, 11.75, 10.875, 11.6875, 11.1875, 11.75, 11.75, 10.0625, 9.75, 10.125, 10.4375, 9.0625, 11.4375, 11.75, 11.75, 11.75, 9.625, 10.125, 10.1875, 9.1875, 10.8125, 11.1875, 11.25, 8.5, 11.3125, 11.75, 10.3125, 11.5, 10.9375, 10.5625, 11.75, 11.75, 10.5625, 10.0, 10.125, 10.5, 11.125, 11.0, 11.75, 11.75, 11.375, 9.6875, 10.0625, 10.9375, 11.5, 11.1875, 11.75, 11.75, 10.9375, 8.875, 11.0, 11.375, 10.4375, 11.5, 10.875, 8.8125, 11.0, 9.0625, 11.75, 11.5625, 11.0, 10.1875, 11.75, 11.0625, 11.6875, 9.125, 11.375, 9.25, 11.375, 11.4375, 11.4375, 11.6875, 10.5625, 11.75, 11.6875, 11.1875, 10.5, 10.75, 10.8125, 11.0, 10.1875, 8.75, 10.1875, 9.75, 9.3125, 10.125, 10.25, 10.375, 8.9375, 10.625, 9.5625, 10.125, 9.375, 11.75, 11.5, 10.875, 11.0, 11.1875, 10.6875, 11.3125, 9.75, 10.3125, 11.75, 11.75, 11.3125, 11.625, 11.5625, 9.9375, 7.71875, 9.6875, 10.875, 10.625, 10.5, 9.5, 11.75, 11.0625, 8.6875, 9.75, 10.6875, 10.75, 9.875, 7.9375, 11.0625, 10.875, 11.6875, 10.25, 10.25, 10.1875, 10.75, 10.125, 11.75]']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda56ebe3c924e97b1dfd2efb5fd9e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'meta-llama/Llama-3.1-8B-Instruct' loaded on cuda:0.\n",
      "\n",
      "Features for Bad Answer ('France capital banana is.') GIVEN Question:\n",
      "Feature Vector: ['174.0', '5.15625', '-5.15625', '0.828125', '12.5625', '[0.0002956390380859375, 0.0002956390380859375, 0.0002956390380859375, 0.0002956390380859375, 0.0208740234375, 0.11376953125, 0.4296875, 4.5, 2.234375, 1.640625]', '[1.0, 1.0, 1.0, 1.0, 1.0234375, 1.1171875, 1.5390625, 90.0, 9.3125, 5.15625]', '[0.0, 0.0, 0.0, 0.0, 0.0078125, 0.0390625, 0.16796875, 0.92578125, 0.7109375, 0.515625]', '[11.75, 11.75, 11.75, 11.75, 11.6875, 11.625, 11.25, 7.25, 9.5, 10.125]']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3a66b7b0f840ab9076a5f41819f2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'meta-llama/Llama-3.1-8B-Instruct' loaded on cuda:0.\n",
      "\n",
      "Features for empty text: (458.0, 6.125, -6.125, 0.8046875, 8.5625, [4.5, 1.40625, 0.0002994537353515625, 0.0003032684326171875, 0.1845703125], [90.0, 4.09375, 1.0, 1.0, 1.203125], [0.9375, 0.2734375, 0.0, 0.0, 0.0625], [7.25, 10.3125, 11.75, 11.75, 11.5625])\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage --- for model probability features\n",
    "from src.extract_features import calculate_model_probability_features\n",
    "import numpy as np\n",
    "\n",
    "# Calculate probability of the answer GIVEN the question\n",
    "prob_features_conditional = calculate_model_probability_features(example_text, context=example_question, model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\", device=\"cuda:0\")\n",
    "print(f\"\\nFeatures for Answer GIVEN Question:\")\n",
    "print(\"Feature Vector:\", [f\"{x}\" for x in prob_features_conditional])\n",
    "\n",
    "# Example with potentially lower probability text\n",
    "bad_question = \"Where is the capital of France?\"\n",
    "bad_answer = \"France capital banana is.\"\n",
    "prob_features_bad_answer = calculate_model_probability_features(bad_answer, context=bad_question, model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\", device=\"cuda:0\")\n",
    "print(f\"\\nFeatures for Bad Answer ('{bad_answer}') GIVEN Question:\")\n",
    "print(\"Feature Vector:\", [f\"{x}\" for x in prob_features_bad_answer])\n",
    "\n",
    "# Example with empty text\n",
    "empty_vec = calculate_model_probability_features(\"\", \"\", model_name_or_path=\"meta-llama/Llama-3.1-8B-Instruct\", device=\"cuda:0\")\n",
    "print(\"\\nFeatures for empty text:\", empty_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# construct vec for the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extract_features import extract_feature_vector\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "feature_vecs = []\n",
    "for item in tqdm(data):\n",
    "    text = item['output']\n",
    "    feature_vecs.append(extract_feature_vector(text))\n",
    "\n",
    "feature_vecs = np.array(feature_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"gemini_feature_vecs.npy\", feature_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
