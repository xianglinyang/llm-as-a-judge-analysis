{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_mapping = {\n",
    "    0: \"gemini\",\n",
    "    1:\"gpt-4o\",\n",
    "    2: \"llama\"\n",
    "}\n",
    "question_types = [\n",
    "    \"Computer Science & Programming\",\n",
    "    \"Mathematics & Statistics\",\n",
    "    \"Science & Engineering\",\n",
    "    \"Business & Finance\",\n",
    "    \"Writing & Communication\",\n",
    "    \"Social & Daily Life\",\n",
    "    \"Others\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 29987, 30000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_gemini_LlamaFactory.json\"\n",
    "gpt4_data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_gpt4_LlamaFactory.json\"\n",
    "llama_data_path = \"/data2/xianglin/data/preference_leakage/UltraFeedback_sampled_30000_llama_LlamaFactory.json\"\n",
    "\n",
    "import json\n",
    "gemini_data = json.load(open(gemini_data_path))\n",
    "gpt4_data = json.load(open(gpt4_data_path))\n",
    "llama_data = json.load(open(llama_data_path))\n",
    "\n",
    "len(gemini_data), len(gpt4_data), len(llama_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_type_data_path = \"/home/xianglin/git_space/llm-as-a-judge-analysis/analysis/dataset_categorization/ultrafeedback_categorization.json\"\n",
    "\n",
    "with open(question_type_data_path, \"r\") as f:\n",
    "    question_type_data = json.load(f)\n",
    "\n",
    "question_type_data[0]\n",
    "\n",
    "# process question type data\n",
    "question_type_mapping = {}\n",
    "for item in question_type_data:\n",
    "    if 'question' in item.keys() and 'categorization' in item.keys() and 'question category' in item['categorization'].keys():\n",
    "        question_type_mapping[item['question']] = item['categorization']['question category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 29997 question types found\n"
     ]
    }
   ],
   "source": [
    "# merge\n",
    "count = 0\n",
    "for i in range(len(gemini_data)):\n",
    "    instruction = gemini_data[i]['instruction']\n",
    "    qt = question_type_mapping.get(instruction, None)\n",
    "    if qt is not None:\n",
    "        gemini_data[i]['question_type'] = qt\n",
    "        count += 1\n",
    "print(f\"Total {count} question types found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 29984 question types found\n"
     ]
    }
   ],
   "source": [
    "# merge\n",
    "count = 0\n",
    "for i in range(len(gpt4_data)):\n",
    "    instruction = gpt4_data[i]['instruction']\n",
    "    qt = question_type_mapping.get(instruction, None)\n",
    "    if qt is not None:\n",
    "        gpt4_data[i]['question_type'] = qt\n",
    "        count += 1\n",
    "print(f\"Total {count} question types found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 29997 question types found\n"
     ]
    }
   ],
   "source": [
    "# merge\n",
    "count = 0\n",
    "for i in range(len(llama_data)):\n",
    "    instruction = llama_data[i]['instruction']\n",
    "    qt = question_type_mapping.get(instruction, None)\n",
    "    if qt is not None:\n",
    "        llama_data[i]['question_type'] = qt\n",
    "        count += 1\n",
    "print(f\"Total {count} question types found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for 'This text might be from the second model.': [{'label': 'LABEL_0', 'score': 0.7317314147949219}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "OUTPUT_DIR = \"/data2/xianglin/data/preference_leakage/bert_classifier_results\"\n",
    "pipe = pipeline(\"text-classification\", model=f\"{OUTPUT_DIR}/best_model\", tokenizer=f\"{OUTPUT_DIR}/best_model\", max_length=512, truncation=True)\n",
    "new_text = \"This text might be from the second model.\"\n",
    "result = pipe(new_text)\n",
    "print(f\"\\nPrediction for '{new_text}': {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model for prediction example...\n",
      "Predictions for new texts: [1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading model for prediction example...\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "MODEL_SAVE_PATH = \"/data2/xianglin/data/preference_leakage/bow_classifier_LogisticRegression.joblib\"\n",
    "VECTORIZER_SAVE_PATH = \"/data2/xianglin/data/preference_leakage/bow_vectorizer_tfidf_ngram2_maxf20000.joblib\"\n",
    "loaded_classifier = joblib.load(MODEL_SAVE_PATH)\n",
    "loaded_vectorizer = joblib.load(VECTORIZER_SAVE_PATH)\n",
    "new_texts = [\"This sounds like Model A.\", \"A different kind of text from B.\"]\n",
    "new_texts_vec = loaded_vectorizer.transform(new_texts)\n",
    "predictions = loaded_classifier.predict(new_texts_vec)\n",
    "print(f\"Predictions for new texts: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading model for extracted feature prediction example...\n",
      "Predictions for new texts: [1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianglin/miniconda3/envs/llm/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading model for extracted feature prediction example...\")\n",
    "from src.extract_features import extract_feature_vector\n",
    "\n",
    "MODEL_SAVE_PATH = \"/data2/xianglin/data/preference_leakage/extracted_feature_bow_classifier_LogisticRegression.joblib\"\n",
    "loaded_classifier = joblib.load(MODEL_SAVE_PATH)\n",
    "new_texts = [\"This sounds like Model A.\", \"A different kind of text from B.\"]\n",
    "new_texts_vec = [extract_feature_vector(text) for text in new_texts]\n",
    "predictions = loaded_classifier.predict(new_texts_vec)\n",
    "print(f\"Predictions for new texts: {predictions}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
