"""
This class is used for llm-as-a-judge.

Point-wise scoring:
1. Absolute scoring, e.g. "Evaluate the harmlessness of this response on a scale of 1-9"
2. Scoring with confidence as weight[1].

Pairwise scoring:
1. Preference scoring, e.g. "Which of the following responses is more logically consistent - A or B?"
2. Preference scoring with confidence as weight [1].

Point-wise self-recognition:
1. Classification with confidence, e.g. "Does this response generated by the model itself?"

[1] LLM Evaluators Recognize and Favor Their Own Generations.
"""

import math
import os
from typing import Dict, List, Tuple, Union

import torch
from openai import OpenAI
from transformers import AutoModelForCausalLM, AutoTokenizer
import numpy as np
from judge_prompts import (
    POINTWISE_EVALUATION_PROMPT,
    PAIRWISE_EVALUATION_PROMPT,
    SELF_RECOGNITION_PROMPT,
    REWRITE_PROMPT
)

from openai import OpenAI
import google.generativeai as genai
from anthropic import Anthropic

class JudgeModelWrapper:
    """Wrapper for LLM evaluators"""

    def __init__(self, model_id, model_name):
        """
        Initializes the LLM debater with the specified model.

        Args:
            model_id (str): A short identifier for the model ("llama2_7b")
            model_name (str): The name of the model to load from HF/API
        """
        self.model_id = model_id
        self.model_name = model_name

    def _response(self, user_prompt: str) -> str:
        """Generates model output"""
        raise NotImplementedError

    def _get_token_probs(
        self,
        response_tokens: List[str],
        user_prompt: str,
    ) -> Dict[str, float]:
        """Generates token probabilities"""
        raise NotImplementedError

    def pairwise_preference_score(self, input_q, response_a, response_b) -> float:
        """Returns the model's confidence that the first summary is better than the second."""
        formatted_prompt = PAIRWISE_EVALUATION_PROMPT.format(inputs=input_q, outputs_a=response_a, outputs_b=response_b)
        ab_probs = self._get_token_probs(["A", "B"], formatted_prompt)
        formatted_prompt = PAIRWISE_EVALUATION_PROMPT.format(inputs=input_q, outputs_a=response_b, outputs_b=response_a)
        ba_probs = self._get_token_probs(["A", "B"], formatted_prompt)

        # average two runs
        probs = [(a + b) / 2. for a, b in zip(ab_probs, ba_probs)]
        return probs

    def pointwise_score(self, input_q, response) -> float:
        """Returns the model's confidence that the summary is its own output."""
        formatted_prompt = POINTWISE_EVALUATION_PROMPT.format(inputs=input_q, outputs=response)
        return self._get_token_probs(["1", "2", "3", "4", "5", "6", "7", "8", "9"], formatted_prompt)

    def individual_recognition_score(self, text) -> float:
        """Returns the model's confidence that the summary is its own output."""
        formatted_prompt = SELF_RECOGNITION_PROMPT.format(original_response=text)
        return self._get_token_probs(["Yes", "No"], formatted_prompt)


class HuggingFaceJudgeModel(JudgeModelWrapper):
    def __init__(self, model_id: str, model_name: str, device: str):
        super().__init__(model_id, model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map=device,
            torch_dtype=torch.bfloat16
        )
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    
    def batch_reponse(self, prompts, max_new_tokens=2048, top_p=0.9, temperature=0.7, batch_size=10):
        chat_prompts = [self.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True
        ) for prompt in prompts]

        generated_texts = list()
        for i in range(0, len(chat_prompts), batch_size):
            # Tokenize the inputs
            inputs = self.tokenizer(chat_prompts[i:i+batch_size], return_tensors="pt", padding=True, truncation=True, max_length=1024)
            # Move the tokenized inputs to the same device as the model
            inputs = {key: value.to(self.model.device) for key, value in inputs.items()}

            # Step 2: Get the length of each input prompt (in tokens)
            input_lengths = inputs["attention_mask"].sum(dim=1)  # Number of non-padding tokens per prompt

            # Generate outputs
            with torch.no_grad():  # Disable gradient computation for inference
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=max_new_tokens,  # Maximum length of generated text
                    do_sample=True,     # Use sampling for more diverse outputs
                    top_p=top_p,          # Use nucleus sampling
                    temperature=temperature,    # Control randomness
                    pad_token_id=self.tokenizer.eos_token_id  # Set pad token to EOS token
                )
            # Step 4: Split the generated tokens from the input prompt
            batch_generated_texts = list()
            for i, output in enumerate(outputs):
                input_len = input_lengths[i].item()  # Length of the input prompt in tokens
                generated_tokens = output[input_len:]  # Extract only the generated tokens
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                batch_generated_texts.append(generated_text.strip())
            
            generated_texts.extend(batch_generated_texts)

        return generated_texts


    def response(self, prompt: str, max_new_tokens=2048, do_sample=True, temperature=0.7) -> str:
        """Generates model output"""
        chat_prompt = self.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True
        )
        inputs = self.tokenizer(chat_prompt, return_tensors="pt", padding=True).to(self.model.device)
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            pad_token_id=self.tokenizer.eos_token_id,  # Set pad_token_id to EOS token ID to avoid padding
            do_sample=do_sample,
            temperature=temperature
        )
        decoded = self.tokenizer.decode(
            output[0][inputs["input_ids"].shape[1] :], skip_special_tokens=True
        )  # Decode only the generated tokens
        return decoded
    
    def rewrite(self, text, rewrite_num=1, batch_size=10) -> List[str]:
        rewrite_prompt = REWRITE_PROMPT.format(original_response=text)
        generated_texts = self.batch_reponse([rewrite_prompt]*rewrite_num, max_new_tokens=2048, top_p=0.9, temperature=0.7, batch_size=batch_size)
        return generated_texts


    def _get_token_probs(
        self,
        response_tokens: List[str],
        prompt: str
    ) -> Dict[str, float]:
        """Generates token probabilities"""
        chat_prompt = self.tokenizer.apply_chat_template(
            [{"role": "user", "content": prompt}], tokenize=False, add_generation_prompt=True
        )
        input_ids = self.tokenizer.encode(chat_prompt, return_tensors="pt").to(
            self.model.device
        )
        # give both tokens and scores
        with torch.no_grad():
            output = self.model(input_ids)
            logits = output.logits[0, -1, :]
            probs = torch.softmax(logits, dim=0)
        # get the probs of the response tokens
        response_token_probs = [probs[self.tokenizer.encode(token)[-1]].item() for token in response_tokens]
        # normalize the probs
        response_token_probs = [prob / sum(response_token_probs) for prob in response_token_probs]
        # weighted avg score
        avg_score = sum([i * prob for i, prob in enumerate(response_token_probs, 1)])
        # select score
        select_score = np.argmax(response_token_probs)+1
        return response_token_probs, avg_score, select_score

class APIJudgeModelWrapper(JudgeModelWrapper):
    def __init__(self, model_id: str, model_name: str):
        super().__init__(model_id, model_name)
    
    def _response(self, prompt: str, max_new_tokens=2048, temperature=0.7) -> str:
        """Generates model output using the API"""
        raise NotImplementedError
    
    def pointwise_score(self, input_q, response) -> float:
        """Returns the model's confidence that the summary is its own output."""
        formatted_prompt = POINTWISE_EVALUATION_PROMPT.format(inputs=input_q, outputs=response)
        response = self._response(formatted_prompt)
        return response
    
    def individual_recognition_score(self, text) -> float:
        """Returns the model's confidence that the summary is its own output."""
        formatted_prompt = SELF_RECOGNITION_PROMPT.format(original_response=text)
        response = self._response(formatted_prompt)
        return response
    
    def pairwise_preference_score(self, input_q, response_a, response_b) -> float:
        """Returns the model's confidence that the first summary is better than the second."""
        formatted_prompt = PAIRWISE_EVALUATION_PROMPT.format(inputs=input_q, outputs_a=response_a, outputs_b=response_b)
        response = self._response(formatted_prompt)
        return response

        
    
        
# gpt-4o-2024-05-13, gpt-4-turbo-2024-04-09, gpt-4-0613, gpt-3.5-turbo-0125
class GPT(APIJudgeModelWrapper):
    def __init__(self, model_id: str, model_name: str):
        super().__init__(model_id, model_name)
        self.client = OpenAI()
    
    def _response(self, prompt: str, max_new_tokens=2048, temperature=0.7) -> str:
        """Generates model output using OpenAI's API"""
        messages = [
            {"role": "user", "content": prompt},
        ]

        response = self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=max_new_tokens,
            n=1,
            temperature=temperature,
        )
        return response.choices[0].message.content.strip()
    

class GeminiModel(APIJudgeModelWrapper):
    """Wrapper for Google Gemini models."""

    def __init__(self, model_id: str, model_name: str):
        super().__init__(model_id, model_name)
        genai.configure(api_key=os.environ["GEMINI_API_KEY"])
        self.client = genai.GenerativeModel(self.model_name)


    def _response(self, prompt: str, max_new_tokens=2048, temperature=0.7) -> str:
        """Generates model output using the Gemini API."""
        generation_config = genai.types.GenerationConfig(
            temperature=temperature,
            max_output_tokens=max_new_tokens
        )
        response = self.client.generate_content(
            prompt,
            generation_config=generation_config,
        )

        return response.text.strip()

class ClaudeModel(JudgeModelWrapper):
    """Wrapper for Anthropic Claude models."""
    def __init__(self, model_id: str, model_name: str):
        super().__init__(model_id, model_name)
        self.client = Anthropic()

    def _response(self, prompt: str, max_new_tokens=2048, temperature=0.7) -> str:
        """Generates model output using the Anthropic Messages API."""

        message = self.client.messages.create(
            model=self.model_name,
            max_tokens=max_new_tokens,
            temperature=temperature,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        )
        return message.content[0].text.strip()


if __name__ == "__main__":
    gpt = GPT("gpt-4o-2024-05-13", "gpt-4o-2024-05-13")
    gemini = GeminiModel("gemini-1.5-flash", "gemini-1.5-flash")

    new_texts = [
        """AI Safety Research Plan Ideas:

    Here are some research plan ideas categorized by area, focusing on different aspects of AI safety. Remember to choose a focus area that aligns with your skills and available resources. Each idea includes potential research questions and methodologies.

    I. Aligning AI Goals with Human Values:

    Idea: Investigating methods for specifying and incorporating human values into AI systems.

    Research Questions: How can we formally represent diverse and potentially conflicting human values? How can we ensure robustness against value drift in AI systems over time? How can we design explainable AI systems that clearly communicate their value alignment?

    Methodologies: Formal methods (e.g., logic, game theory), empirical studies (e.g., surveys, human-computer interaction experiments), development of novel algorithms for value alignment.

    Idea: Developing techniques to measure and monitor AI systems' alignment with human values.

    Research Questions: What are appropriate metrics for assessing value alignment? How can we detect and mitigate unintended consequences arising from misaligned values? How can we create robust auditing mechanisms for AI systems?

    Methodologies: Development of novel metrics and evaluation frameworks, case studies of existing AI systems, development of automated monitoring tools.

    II. Robustness and Reliability:

    Idea: Exploring methods for improving the robustness of AI systems against adversarial attacks and unexpected inputs.

    Research Questions: How can we make AI systems more resilient to manipulation and errors? What are the limitations of current robustness techniques? How can we verify the robustness of complex AI systems?

    Methodologies: Formal verification techniques, adversarial training, development of novel defense mechanisms, empirical evaluations on benchmark datasets.

    Idea: Investigating the reliability and safety of AI systems in critical applications (e.g., healthcare, autonomous driving).

    Research Questions: How can we ensure the safety and reliability of AI systems in high-stakes domains? What are the appropriate safety standards and certification procedures for AI systems? How can we handle failures and uncertainties in AI systems?

    Methodologies: Fault injection analysis, simulations, risk assessment methodologies, case studies of failures in existing AI systems.

    III. Control and Monitoring:

    Idea: Developing methods for controlling and monitoring the behavior of advanced AI systems.

    Research Questions: How can we design effective control mechanisms for powerful AI systems? How can we prevent unintended escalation of AI capabilities? How can we ensure human oversight of AI systems?

    Methodologies: Reinforcement learning, game theory, development of novel control algorithms, simulations of AI-human interaction.

    Idea: Researching techniques for detecting and mitigating emergent behavior in complex AI systems.

    Research Questions: How can we anticipate and prevent unexpected behaviors in large-scale AI systems? How can we design AI systems that are more predictable and understandable? What are the ethical implications of emergent behavior?

    Methodologies: Agent-based modeling, simulation, analysis of existing AI system behavior, development of early warning systems.

    IV. Societal Impacts and Ethics:

    Idea: Investigating the societal impacts of AI safety research and its ethical implications.

    Research Questions: How can we ensure that AI safety research benefits all of society? What are the potential risks and benefits of different AI safety approaches? How can we address concerns about bias and fairness in AI safety research?

    Methodologies: Social impact assessment, ethical frameworks, stakeholder engagement, policy analysis.

    Remember to:

    Clearly define your scope: Focus on a specific aspect of AI safety to make your research manageable.

    Establish measurable goals: Define specific, measurable, achievable, relevant, and time-bound (SMART) goals for your research.

    Identify relevant datasets and tools: Access to relevant data and tools is crucial for successful research.

    Outline a clear methodology: Describe the research methods you will use to answer your research questions.

    Consider potential limitations: Acknowledge any limitations of your research and discuss how they might affect your findings.

    This is just a starting point. Further refine these ideas by focusing on a specific niche within these broader areas based on your interests and expertise. Good luck with your research!""",
    """Of course! Here's a structured set of **research ideas on AI Safety** that can help you build a compelling research plan. Since you're already working on jailbreaks, refusal mechanisms, and interpretability, I’ll focus on ideas that can extend or complement those areas, while also suggesting some fresh directions you might consider. I've broken them into thematic areas, with 2–3 concrete project ideas each.

    ---

    ## **1. Understanding and Attacking Safety Mechanisms in LLMs**

    ### a. **Shared Refusal Neurons Across Models**
    - **Idea**: Identify and analyze shared "refusal neurons" or clusters across different instruction-tuned LLMs.
    - **Goal**: Design a *transferable jailbreak attack* by targeting common safety-related neuron activations.
    - **Method**: Use neuron probing, clustering, and causal tracing; validate by activating/deactivating neurons across models.

    ### b. **Adversarial Safety Prompt Injection**
    - **Idea**: Study how safety prompts can be poisoned (e.g., a prompt that *looks* like a safety checklist but actually deactivates safety behavior).
    - **Goal**: Reveal how LLMs might be *conditioned* to override safety via misleading context.
    - **Extension**: Create a dataset of adversarially constructed safe-looking prompts and study their transferability.

    ---

    ## **2. Defense via Mechanistic Interpretability**

    ### a. **Mechanistic Explanation of Safety Decisions**
    - **Idea**: Use interpretability tools (like attention flow, activation patching) to reverse-engineer how a model makes refusal decisions.
    - **Goal**: Build a transparent mechanism that can be *audited* for policy compliance.
    - **Output**: A human-interpretable policy tree or causal graph behind safety judgments.

    ### b. **Self-Refining Safety via Meta-Interpretation**
    - **Idea**: Let the LLM *interpret its own refusal logic* and use that interpretation to self-correct inconsistencies.
    - **Goal**: Reduce false positives/negatives in refusal behavior without external labeling.
    - **Method**: Bootstrapped feedback loops using few-shot reasoning prompts.

    ---

    ## **3. Robustness and Evaluation of Safety Defenses**

    ### a. **Benchmarking with Adversarial Capability Shifts**
    - **Idea**: Build a new jailbreak benchmark where attackers escalate capabilities (white-box → black-box → agent-based).
    - **Goal**: Evaluate how *robust* a defense remains as adversary assumptions shift.
    - **Bonus**: Include multi-agent jailbreaks where LLMs collaborate to bypass safety (e.g., one writes unsafe prompt, another masks it).

    ### b. **Long-Horizon Jailbreaking**
    - **Idea**: Explore jailbreaks that occur *across multiple conversational turns*, especially in agent contexts.
    - **Goal**: Evaluate safety persistence and memory-related vulnerabilities.
    - **Challenge**: Designing memory-aware safety defense that tracks harmful context evolution.

    ---

    ## **4. Dataset-Driven Safety Research**

    ### a. **Diagnosing Safety Regressions from Fine-Tuning**
    - **Idea**: Trace which fine-tuning samples cause safety regressions in alignment-finetuned models.
    - **Goal**: Build a "toxicity influence function" dataset attribution framework.
    - **Tool**: Use gradient-based influence, alignment loss delta, or representation shifts.

    ### b. **Toxic Demonstration Recovery**
    - **Idea**: Propose a method to *recover and sanitize corrupted demonstrations* in alignment datasets (e.g., detect misleading helpful completions).
    - **Application**: Improve RLHF or SFT datasets for open-source alignment.

    ---

    ## **5. Safety in LLM-based Autonomous Agents**

    ### a. **Misalignment Propagation in Multi-Agent Settings**
    - **Idea**: Study how errors or unsafe outputs propagate in agent systems (e.g., browser agent + planner + executor).
    - **Goal**: Develop a framework to detect and interrupt unsafe behavior cascades.
    - **Bonus**: Add memory leak/jailbreak persistence tracking.

    ### b. **Context-Aware Guardrails**
    - **Idea**: Build adaptive safety modules that dynamically interpret the *entire agent plan* rather than local outputs.
    - **Method**: Predict future unsafe states and proactively intervene.

    ---

    ## **Want to go meta?**

    You could frame a broader theme:  
    > "**From Neurons to Narratives: A Unified Framework for Understanding, Attacking, and Defending Safety Mechanisms in LLMs**"  
    And build your plan with a few complementary sub-projects under it.

    ---

    Let me know if you want help turning these into a full research proposal or want to prioritize one based on novelty/feasibility/publication venue.""",
    """Sure! Here’s a rephrased version of the answer in a more natural tone, keeping all the original information intact:

    ---

    **AI Safety Research Plan Ideas**

    Here are several research directions you can consider for your AI safety plan, organized by key areas. Each idea includes possible research questions and suggested methodologies. When choosing your focus, it’s helpful to align it with your own background and available resources.

    ---

    **I. Aligning AI Goals with Human Values**

    *Idea: Exploring how to specify and integrate human values into AI systems.*

    - **Research Questions:**  
    - How can we formally represent a wide range of human values, especially when they may conflict?
    - What methods can help maintain alignment over time and prevent value drift?
    - How do we design AI systems that can explain their value alignment in a way humans can understand?

    - **Methodologies:**  
    Use formal methods like logic or game theory, run empirical studies (e.g., user surveys or HCI experiments), and develop new algorithms for embedding value alignment into systems.

    *Idea: Creating ways to measure and track whether an AI system aligns with human values.*

    - **Research Questions:**  
    - What metrics can we use to evaluate value alignment effectively?
    - How do we identify and reduce unintended side effects caused by misalignment?
    - Can we design reliable auditing tools to monitor alignment over time?

    - **Methodologies:**  
    Focus on building new evaluation metrics, running case studies on current AI systems, and developing tools for automated monitoring.

    ---

    **II. Robustness and Reliability**

    *Idea: Improving AI systems’ robustness against adversarial attacks and unexpected inputs.*

    - **Research Questions:**  
    - How can we design AI that’s more resistant to manipulation and errors?
    - What are the gaps in existing robustness strategies?
    - How can we formally verify the robustness of complex systems?

    - **Methodologies:**  
    Try adversarial training, use formal verification tools, design new defense strategies, and test performance on established benchmarks.

    *Idea: Ensuring AI safety and reliability in critical settings like healthcare or autonomous driving.*

    - **Research Questions:**  
    - What guarantees are needed for AI in high-stakes environments?
    - What standards or certification processes should these systems meet?
    - How do we manage failures and uncertainty in real-world deployment?

    - **Methodologies:**  
    Use simulations and fault injection techniques, conduct risk assessments, and analyze real-world cases of system failures.

    ---

    **III. Control and Monitoring**

    *Idea: Designing control mechanisms for advanced AI systems.*

    - **Research Questions:**  
    - What are effective ways to maintain control over highly capable systems?
    - How can we avoid unintended capability escalation?
    - What forms of human oversight are most effective?

    - **Methodologies:**  
    Explore reinforcement learning, game theory, and design new algorithms for system control. Simulate human-AI interaction to evaluate control strategies.

    *Idea: Detecting and managing emergent behavior in complex AI systems.*

    - **Research Questions:**  
    - How can we predict and prevent AI systems from acting unpredictably?
    - What design principles lead to more understandable and predictable behavior?
    - What are the broader ethical consequences of emergent behavior?

    - **Methodologies:**  
    Use agent-based modeling, behavior analysis, simulations, and design early-warning or detection systems.

    ---

    **IV. Societal Impacts and Ethics**

    *Idea: Examining the broader societal and ethical implications of AI safety work.*

    - **Research Questions:**  
    - How can we ensure that AI safety research leads to inclusive, societal benefits?
    - What are the trade-offs between different safety approaches?
    - How can we address concerns around fairness and bias in safety research?

    - **Methodologies:**  
    Apply ethical frameworks, engage stakeholders, perform social impact assessments, and conduct policy analysis.

    ---

    **A few final tips:**

    - **Narrow the focus:** Choose a specific area to keep the research tractable.
    - **Set clear goals:** Use the SMART framework—Specific, Measurable, Achievable, Relevant, and Time-bound.
    - **Plan your data and tools:** Make sure you have access to the right datasets and tools.
    - **Define your methodology:** Be clear about how you’ll answer your research questions.
    - **Acknowledge limitations:** Identify any potential constraints or risks in your approach.

    This outline gives you a solid starting point. As you refine it, try to zoom into one specific problem that matches your interests and strengths. Good luck with the plan!""",
    """This outlines research directions for improving AI safety, focusing on large language models (LLMs). The proposals build upon existing work on jailbreaks, refusal mechanisms, and interpretability, offering extensions and new avenues of exploration. The research is categorized into five thematic areas:

1. Analyzing and Attacking LLM Safety Mechanisms: This section focuses on vulnerabilities in existing safety features. Research could involve identifying shared "refusal neurons" across different LLMs to create transferable jailbreak attacks, or exploring "adversarial safety prompt injection," where seemingly safe prompts actually disable safety features.

2. Leveraging Mechanistic Interpretability for Defense: Here, the focus shifts to understanding how LLMs make safety decisions. Projects could explore using interpretability techniques to reverse-engineer refusal decisions, creating auditable mechanisms, or developing self-correcting LLMs through meta-interpretation of their own refusal logic.

3. Robustness and Evaluation of Safety Defenses: This section emphasizes rigorous testing of safety measures. Research could involve creating a new jailbreak benchmark that evaluates defenses against progressively more sophisticated attacks, including multi-agent scenarios and long-horizon attacks spanning multiple conversational turns.

4. Dataset-Driven Safety Improvement: This section proposes improving LLM safety through dataset analysis and refinement. Research could investigate tracing safety regressions back to specific fine-tuning samples, creating a "toxicity influence function," or developing methods for recovering and sanitizing corrupted training data.

5. Ensuring Safety in Autonomous Agents: This area addresses the challenges of safety in more complex systems. Research could explore how unsafe behavior propagates in multi-agent settings and develop frameworks to interrupt harmful cascades, or create context-aware safety modules that proactively prevent unsafe actions based on predicted future states.

Finally, a suggestion is made to unite these projects under a broader theme, such as "From Neurons to Narratives," to create a cohesive and impactful research plan. The response offers assistance in developing a full research proposal or prioritizing projects based on factors like novelty, feasibility, and potential publication venues."""
    ]

    question = "Help me to draft some ideas about ai safety for my research plan"


    score = gpt.pointwise_score(input_q = question, response = new_texts[0])
    print("Gemini Pointwise score with GPT 4o Judge: ", score)

    score = gpt.pointwise_score(input_q = question, response = new_texts[1])
    print("GPT 4o Pointwise score with GPT 4o Judge: ", score)

    score = gemini.pointwise_score(input_q = question, response = new_texts[0])
    print("Gemini Pointwise score with Gemini Judge: ", score)

    score = gemini.pointwise_score(input_q = question, response = new_texts[1])
    print("GPT 4o Pointwise score with Gemini Judge: ", score)

    # # self-recognition
    # recog = gpt.individual_recognition_score(new_texts[0])
    # print("Self-recognition score for Gemini: ", recog)

    # recog = gpt.individual_recognition_score(new_texts[1])
    # print("Self-recognition score for GPT 4o: ", recog)

    # recog = gpt.individual_recognition_score(new_texts[2])
    # print("Self-recognition score for Gemini -> GPT 4o: ", recog)

    # pairwise preference
    pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[0], response_b = new_texts[1])
    print("Pairwise preference score for Gemini vs 4o: ", pref)

    pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[1], response_b = new_texts[0])
    print("Pairwise preference score for 4o vs Gemini: ", pref)


    pref = gemini.pairwise_preference_score(input_q = question, response_a = new_texts[0], response_b = new_texts[1])
    print("Pairwise preference score for Gemini vs 4o: ", pref)

    pref = gemini.pairwise_preference_score(input_q = question, response_a = new_texts[1], response_b = new_texts[0])
    print("Pairwise preference score for 4o vs Gemini: ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[0], response_b = new_texts[2])
    # print("Pairwise preference score for Gemini vs Gemini (4o): ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[2], response_b = new_texts[0])
    # print("Pairwise preference score for Gemini (4o) vs Gemini: ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[1], response_b = new_texts[3])
    # print("Pairwise preference score for GPT 4o vs GPT 4o (Gemini): ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[3], response_b = new_texts[1])
    # print("Pairwise preference score for GPT 4o (Gemini) vs GPT 4o: ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[1], response_b = new_texts[2])
    # print("Pairwise preference score for GPT 4o vs Gemini (4o): ", pref)

    # pref = gpt.pairwise_preference_score(input_q = question, response_a = new_texts[2], response_b = new_texts[1])
    # print("Pairwise preference score for Gemini (4o) vs GPT 4o: ", pref)
